---
title: 'A look at the Vancouver mayoral race twitter picture'
author: ''
date: '2018-10-01'
slug: vancouver-mayor-race-twitter
twitterImg: post/2018-10-01-a-look-at-the-vancouver-mayor-twitter-picture_files/figure-html/sentiment1-1.png
description: "Using Twitter data and NLP tools to look at the timelines of the six most prominent candidates for Mayor of Vancouver"
categories:
  - blog
  - analysis
tags:
  - politics
  - nlp
  - r
  - twitter
  - vancouver
draft: no
---
```{r include=FALSE, message=FALSE}
library(dplyr)
library(lubridate)
library(rtweet)
mayor_candidates <- c("DavidChenTweets",
                        "hectordbremner",
                        "kennedystewart",
                        "kensimformayor",
                        "ShaunaSylvester",
                        "WaiYoung")

mayor_timelines <- get_timelines(user = mayor_candidates, n = 10000) %>% 
  mutate(date = date(created_at),
         name = case_when(
          screen_name == "DavidChenTweets" ~ "Chen",
          screen_name == "hectordbremner" ~ "Bremner",
          screen_name == "kennedystewart" ~ "Stewart",
          screen_name == "kensimformayor" ~ "Sim",
          screen_name == "ShaunaSylvester" ~ "Sylvester",
          screen_name == "WaiYoung" ~ "Young")) %>% 
  filter(year(date) == 2018,
         month(date) %in% c(4:10))
```

> (Updated October 16, 2018) With the election coming up in the next week and this page getting more traffic, I decided to do a quick refresh of the data used in this post up to and including October 15. Changes are relatively few, and as this site is a github repo, can be tracked via commits to this post. 

### The 2018 Municipal Election in Vancouver

On October 20, a relatively small portion of Vancouverites will vote to elect a mayor, 10 city councillors, 7 park board commissioners and 9 school trustees. Municipal elections do not receive the fanfare or attention of federal or provincial elections and see lower turnouts, but they are an important determinant towards the policies that affect the everyday lives of city residents. 

With 21 candidates for Mayor and 77 candidates for Council, there's a lot to parse through. The City of Vancouver provides [candidate profiles](https://vancouver.ca/your-government/candidate-profiles.aspx), and most of the candidates and parties who are likely to receive the most votes have already released their platforms. 

While there are certainly a number of interesting and unique independent candidates looking to run for Mayor, all existing polling suggests that there are six candidates with a realistic shot at being elected: Kennedy Stewart (Independent), Ken Sim (NPA), Shauna Sylvester (Independent), Hector Bremner (Yes Vancouver), David Chen (ProVancouver), and Wai Young (CoalitionVan).

### Mayor Twitter 

Campaign financing reforms introduced prior to this election have meant that there is less money available to campaigns, and, therefore, less promotion and advertising of candidates ahead of the election. Recent elections around the world have demonstrated, for better or worse, the effectiveness of social media platforms like Twitter in raising candidate profiles and bringing out supporters. All six top mayoral candidates are relatively active on Twitter, and appear to have increased their online participation with the election drawing nearer. Fortunately, Twitter is the best platform for analysis with it's public facing content, relatively generous public APIs, and a number of excellent tools developed to extract and work with tweet data. 

### Highlights and caveats

There's a few key takeaways from this post that I want to highlight here:

* Until we have election results, Twitter performance is not indicative or predictive of anything in this race, but it is an interesting way to look at things. 
* Twitter engagement does not necessarily mean much on its own as it can reflect engagement with a built-in and captive audience, especially when it comes to messages using specific hashtags. 
* That said, all things considered, as a candidate you would like to see an increasing trend in tweet engagement. 
* Shauna Sylvester is trending in the right direction but has a long ways to go to match Kennedy Stewart's overall numbers. 
* Bremner and Sim need to tweet more, if only so that there's more of a text corpus to analyze. 
* Unfortunately the Twitter API makes it challenging to get the number of replies to a given tweet so we are unable to calculate "ratio" for all of the thousands of posts by the six candidates, but we'll always have this one from Wai Young. 
`r blogdown::shortcode('tweet', '1037388282286419968')`

```{r volume, echo=FALSE, message=FALSE}
library(ggplot2)
library(lato)
library(RcppRoll)

mayor_timelines %>% 
  group_by(name,date) %>% 
  tally() %>% 
  arrange(date) %>% 
  ungroup() %>% 
  group_by(name) %>% 
  mutate(mavg = roll_mean(n, 7, fill = 0, align = "right")) -> totals

ggplot(totals, aes(x = date, y = mavg, group = name, colour = name)) +
  geom_line() + 
  theme_lato() +
  facet_wrap(~name) +
  theme(legend.position = "none") + 
  labs(title = "Candidate tweets and retweets per day", 
       subtitle = "7-day moving average",
       x = "", 
       y = "Tweets", 
       caption = paste0("@dshkol | Tweets extracted on ",Sys.Date()))
```
Of the six, Chen has been the most active tweeter since April 2018 with `r totals %>% group_by(name) %>% tally(n) %>% filter(name == "Chen") %>% pull(nn)` tweets, more than three and five times as many as the next prolific tweeters in Sylvester and Stewart, respectively. Bremner, Sim, and Young, in comparison are much less active tweeters, but all appear to have increased their tweeting as the election draws closer. 

### Engagement trends

We can measure visible engagement by adding up the number of retweets and favourites to look how those tweets are performing, but its not always indicative. A viral bad tweet (see above) will have a huge audience and will inevitably see more visible engagement as someone, somewhere, will find something they like in any opinion. Measuring the engagement rate of tweets requires knowledge about the audience size and reach of each tweet, which is not something available publicly, so this is an imprecise measurement, but it can serve as a proxy for audience growth. 
```{r engagement, echo=FALSE, message=FALSE}
mayor_timelines %>% 
  filter(is_retweet == FALSE) %>% 
  group_by(name, date) %>% 
  summarise(eng = sum(favorite_count, retweet_count)) -> eng

ggplot(eng, aes(x = date, y = eng))  +
  geom_point(alpha = 0.25) + 
  geom_smooth(se = FALSE, color = "#e60050", linetype = 5) + 
  theme_lato() +
  facet_wrap(~name, scales = "free") +
  theme(legend.position = "none") +
  labs(title = "Engagement trends", 
       x = "", 
       y = "RT + Fave per Tweet", 
       caption = paste0("@dshkol | LOESS smoothing fit | Tweets extracted on ",Sys.Date()))
```

### Changing sentiments

Sentiment analysis of Twitter data is a well-traversed form of [analysis](http://www.cs.columbia.edu/~julia/papers/Agarwaletal11.pdf). Briefly, sentiment analysis relies on algorithms that work existing natural language corpuses to estimate the positive or negative score value of words and sentences. The sentiment of a tweet is estimated by adding up the sentiment scores of the individual words within that tweet. A higher score suggests a more positive sentiment. These sentiment scores are useful as a quick meta analysis of the high-level approach each candidate takes in composing their tweets. 
```{r sentiment1, echo = FALSE, message = FALSE}
mayor_timelines %>%
  filter(month(date) %in% c(6:10),
         is_retweet == FALSE) %>%
  mutate(sent = syuzhet::get_sentiment(text, method = "bing")) %>% 
  select(name, date, status_id, sent) -> post_sent

ggplot(post_sent, aes(x = date, y = sent, colour = name)) + 
  geom_jitter(alpha = 0.25) +
  geom_hline(yintercept = 0)+ 
  facet_wrap(~name, scales = "fixed") +
  theme_lato() +
  theme(legend.position = "none") + 
  labs(title = "Vancouver mayoral candidate tweet sentiment scores", 
       x = "", 
       y = "Sentiment score", 
       caption = paste0("@dshkol | Higher values suggest more positive language | Tweets extracted on ",Sys.Date()))
```
While all candidates have tweets classified as positive and negative, it is Chen who is the candidate who appears to rely the most on negative sentiment tweets. 

Both positive and negative messaging are viable approaches to political messaging and revealed trends in tweet sentiment can help inform of us which way candidates appear to be focusing their messaging as the election draws closer. 
```{r sentiment2, echo=FALSE, message=FALSE, warning=FALSE}
mayor_timelines %>%
  filter(month(date) %in% c(6:10),
         is_retweet == FALSE) %>%
  mutate(sent = syuzhet::get_sentiment(text, method = "bing")) %>%
  group_by(name,date) %>%
  summarise(avg_sent = sum(sent, na.rm = TRUE)/n()) %>%
  arrange(date) %>%
  #ungroup() %>%
  group_by(name) %>%
  mutate(mavg = roll_mean(avg_sent, 7, fill = NA)) -> avg_sent

ggplot(avg_sent, aes(x = date, y = mavg))  +
  geom_line() +
  theme_lato() +
  facet_wrap(~name, scales = "fixed") +
  labs(title = "Vancouver mayoral candidate tweet daily sentiment",
      subtitle = "7-day moving average", 
       x = "", 
       y = "Average sentiment score", 
       caption = paste0("@dshkol | Higher values suggest more positive language | Tweets extracted on ",Sys.Date()))
```

There are some well-documented weaknesses in estimating sentiment of human speech. For example, the majority of sentiment lexicons would classify both "bad" and "ass" as words contributing to a negative sentiment, not knowing that "bad ass" actually has a positive connotation. There are ways around this with more sophisticated [n-gram](https://en.wikipedia.org/wiki/N-gram) based sentiment approaches, but this sentiment analysis approaches do not account for this. 

### Tweet text characteristics

Text is structured data with recurring patterns that are typically unique to a person. Even within the confines of 140 or 280 characters, the idiosyncratic linguistic patterns of different tweeters will often show up. The last few years have seen the development of numerous powerful and adaptable tools and libraries for natural language processing (NLP) in R that make it straight-forward to do analysis of large bodies of text at scale. We can use these tools to see if there are distinct patterns in how these six candidate use language on Twitter, and, further down, what kind of language they use. 
```{r features, echo = FALSE, message = FALSE}
library(textfeatures)
library(tidyr)
library(ggalt)

mayor_timelines %>% 
  filter(month(date) %in% c(6:10),
         is_retweet == FALSE) %>% 
  textfeatures(threads = 4, normalize = FALSE) %>% 
  left_join(mayor_timelines %>% 
              select(user_id, name) %>% 
              distinct()) -> mayor_features

# Selected characteristics
mayor_chars <- mayor_features %>%
  group_by(name) %>%
  summarise(
    hashtags = mean(n_hashtags),
    mentions = mean(n_mentions),
    chars = mean(n_chars),
    commas = mean(n_commas),
    exclaims = mean(n_exclaims),
    words = mean(n_words),
    caps = mean(n_caps),
    sent1 = mean(sent_afinn),
    firstp = mean(n_first_person),
    secondp = mean(n_second_person),
    third = mean(n_third_person)
  ) %>%
  ungroup() %>%
  gather(feature, value, hashtags:third) %>%
  mutate(
    feature = case_when(
      feature == "caps" ~ "Caps",
      feature == "chars" ~ "# of chars",
      feature == "commas" ~ "Commas",
      feature == "exclaims" ~ "Exclaim",
      feature == "firstp" ~ "P: 1st",
      feature == "hashtags" ~ "Hashtags",
      feature == "mentions" ~ "Mentions",
      feature == "secondp" ~ "P: 2nd",
      feature == "sent1" ~ "Sentiment",
      feature == "third" ~ "P: 3rd",
      feature == "words" ~ "# of words"
    )
  )

ggplot(mayor_chars, aes(x = reorder(name, value), y = value)) +
  #geom_bar(stat = "identity") + 
  geom_lollipop() +
  coord_flip() + 
  theme_lato() + 
  facet_wrap(~feature, scales = "free") +
  labs(title = "Text feature analysis of candidates", 
       y = "Average per tweet", x = "",
       caption = paste0("@dshkol | Analysis of text features | Tweets extracted on ",Sys.Date()))

```
We already saw earlier that Chen's tweets are, on average, the most negative and Sim's were the most positive. But it is also interesting to see that Chen stands out in a number of other ways as well. His tweets are the longest, the most verbose, the most likely to mention other users, and the most likely to take the third person perspective ("they"). Wai Young likes hashtags and exclamation marks the way Bremner likes commas. While both Stewart and Sylvester are running as independents, the former appears to be far more likely to adopt the first-person perspective in their tweets.

### Tweet language differences

Individual words or word-like components can be extracted from any text corpus, such as tweets, using a process called [tokenization](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html). Again, with the NLP tools at our disposal nowadays, this is straight-forward and is further aided by the existence of Twitter specific [token-parsers](http://www.cs.cmu.edu/~ark/TweetNLP/) lexicons of common stop-words that are not interesting for analysis ("it", "the", etc.). This is less of an issue with mayoral candidates who are more likely than the average Twitter user to post in standard English, but it allows for generalization of this process to other Twitter users who may use non-standard English terms ("smh", "wtf", ":)", etc.). 
```{r text1, echo = FALSE, message=FALSE}
# Tidytext stuff
library(tidytext)
library(stringr)

remove_reg <- "&amp;|&lt;|&gt;"

mayor_text <- mayor_timelines %>% 
    filter(month(date) %in% c(6:10),
           is_retweet == FALSE) %>% 
    select(name, text, favorite_count, retweet_count) %>% 
    filter(!str_detect(text, "^RT")) %>%
    mutate(text = str_remove_all(text, remove_reg)) %>%
    unnest_tokens(word, text, token = "tweets") %>%
    filter(!word %in% stop_words$word,
           !word %in% str_remove_all(stop_words$word, "'"),
           str_detect(word, "[a-z]"))

# Removing hashtags and @s
frequency_adjusted <- mayor_text %>%
  filter(!str_detect(word,"^@"),
         !str_detect(word, "^#")) %>% 
  group_by(name) %>% 
  count(word, sort = TRUE) %>% 
  left_join(mayor_text %>% 
              group_by(name) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total)

top_words_adjusted <- frequency_adjusted %>% 
  group_by(name) %>% 
  top_n(10, freq) %>% 
  arrange(desc(freq)) %>% 
  ungroup() %>% 
  mutate(word = factor(word, unique(word))) %>% 
  ungroup()

ggplot(top_words_adjusted, aes(x = reorder(word, freq), 
                      y = freq, 
                      fill = name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~name, scales = "free", ncol = 3) +
  scale_y_continuous(breaks = NULL) +
  coord_flip() + theme_lato() + 
  labs(title = "Mayoral candidates most frequently used words", 
       x = "", y = "Frequency of non-stopword terms",
       caption = paste0("@dshkol | Excludes stopwords, mentions, and hashtags | Tweets extracted on ",Sys.Date()))
```
Token analysis of the candidates' tweets shows a fair bit of similarity: _Vancouver_, _city_, _people_ regularly appearing as the most frequently used words by almost all candidates. Other differences do appear between candidates, however. Consider Sim's emphasis on _team_, Bremner's _hope_ ... and _prior(?)_, Chen's _tax_ and _money_, and Young's _bike_. Both Stewart and Sylvester really emphasis _housing_ in their tweets, as well as Chen to a lesser extent. 

As this text comes from tweets, we can combine it those tweets' engagement numbers to see if any of these words are more likely to lead to likes than other words for each candidate. 
```{r text2, echo = FALSE, message=FALSE}
# What words lead to retweets and favourites
top_faves <- mayor_text %>%
  filter(!str_detect(word,"^@"),
         !str_detect(word, "^#")) %>% 
  group_by(name, word) %>% 
  add_tally() %>% 
  filter(n > 5) %>% 
  summarise(median_faves = median(favorite_count)) %>% 
  top_n(5, median_faves) %>% 
  arrange(desc(median_faves)) %>% 
  ungroup() %>% 
  mutate(word = factor(word, unique(word))) %>% 
  ungroup()

ggplot(top_faves, aes(x = reorder(word, median_faves), 
                               y = median_faves, 
                               fill = name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~name, scales = "free", ncol = 3) +
  coord_flip() + theme_lato() + 
    labs(title = "Candidate words that earn the most likes on Twitter", 
       x = "", y = "Median # of likes for tweets including word",
       caption = paste0("@dshkol | Excludes stopwords, mentions, and hashtags | Tweets extracted on ",Sys.Date()))
```
A few things immediately stand out that should be obvious to anyone who has paid attention to this race to this point: Stewart's audience cares about the coast and his plan to fight the Kinder Morgan pipeline expansion. Sylvester's audience cares about the Broadway Millennium Line extension. Young's tweets about bike lanes appear to particularly resonate, Sim is earning engagement for being Sim and not for what he tweets, and Bremner needs to tweet more so that his data can be quantitatively meaningful. 

> (Oct.16) Update: not too many changes here with the exception of Sylvester who appears to have greater amounts of positive engagements for her recent "leadership"-focused posts. This coincides with a significant increase in the average number of engagements for Sylvester's posts but it's not obvious whether there is a causal relationship there. 

### A note on data and tools

Putting together the analysis in this post takes a lot less time than it looks like thanks to several highly useful R libraries that do most of the heavy lifting here. Tweets for each candidates' timeline were extracted using the Twitter API and the [rtweet](https://rtweet.info/) package. Sentiment analysis was performed using the [syuzhet](https://cran.r-project.org/package=syuzhet) package, while text feature characteristics were extracted using the aptly named [textfeatures](https://cran.r-project.org/package=textfeatures) package. Finally, words and tokens were extracted from text using the [tidytext](https://cran.r-project.org/package=tidytext) package. The authors of that package have an excellent textbook with instructions and examples that can be viewed for free at https://www.tidytextmining.com. 

As usual, the code for data retrieval, data processing, and for the visuals in this post is available on [Github](https://github.com/dshkol/scratchpad/tree/master/content/post).

